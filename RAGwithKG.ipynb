{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BffzAkksM0BX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "offload_dir = \"./offload\"\n",
        "os.makedirs(offload_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oACIkTxyTlYA"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy_yrXQY51Dw"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.40.1 torch==2.0.1 accelerate==0.29.3 trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDjMCubeZ5nf"
      },
      "outputs": [],
      "source": [
        "!pip install peft neo4j node2vec\n",
        "\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from neo4j import GraphDatabase\n",
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HF model\n",
        "peft_model_id = \"solanaO/llama3-8b-sft-qlora-re\"\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16,\n",
        "  offload_buffers=True\n",
        ").to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id =  tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoMzkk6uaAEv"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iVvHmSlaCau"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"You are an experienced annontator.\n",
        "Extract all entities and the relations between them from the following text.\n",
        "Write the answer as a triple entity1|relationship|entitity2. Give exactly one set of triples and not more than that. This is very important.\n",
        "Do not add anything else.\n",
        "Example Text: Alice is from France.\n",
        "Answer: Alice|is from|France.\n",
        "\"\"\"\n",
        "\n",
        "def create_input_prompt(sample):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\",\"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": sample},\n",
        "        ]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrBw66Ab7M8H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoy3AKEy_Jyd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "csv_file_path = pd.read_csv('Your_file.csv') #Include the data file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG4fSBmI9cJg"
      },
      "outputs": [],
      "source": [
        "df = csv_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlvSgE9q8nY4"
      },
      "outputs": [],
      "source": [
        "# Store all the rows of the 'Sentence' column in a list\n",
        "sentence_list = df['self_text'].tolist()\n",
        "\n",
        "\n",
        "# Print the list to verify\n",
        "print(sentence_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr-6MnSRAils"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dtYzWSw-IvU"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP2Ciqsk_2Hu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from textblob import Word\n",
        "\n",
        "\n",
        "# Define the contractions dictionary and the expand function\n",
        "contractions = {\n",
        "    \"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "    \"aren't\": \"are not / am not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he had / he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he shall / he will\",\n",
        "    \"he'll've\": \"he shall have / he will have\",\n",
        "    \"he's\": \"he has / he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how has / how is / how does\",\n",
        "    \"i'd\": \"i had / i would\",\n",
        "    \"i'd've\": \"i would have\",\n",
        "    \"i'll\": \"i shall / I will\",\n",
        "    \"i'll've\": \"i shall have / i will have\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it had / it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it shall / it will\",\n",
        "    \"it'll've\": \"it shall have / it will have\",\n",
        "    \"it's\": \"it has / it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she had / she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she shall / she will\",\n",
        "    \"she'll've\": \"she shall have / she will have\",\n",
        "    \"she's\": \"she has / she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so as / so is\",\n",
        "    \"that'd\": \"that would / that had\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that has / that is\",\n",
        "    \"there'd\": \"there had / there would\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there has / there is\",\n",
        "    \"they'd\": \"they had / they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they shall / they will\",\n",
        "    \"they'll've\": \"they shall have / they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we had / we would\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what shall / what will\",\n",
        "    \"what'll've\": \"what shall have / what will have\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what has / what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when has / when is\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where has / where is\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who shall / who will\",\n",
        "    \"who'll've\": \"who shall have / who will have\",\n",
        "    \"who's\": \"who has / who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why has / why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you had / you would\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"you shall / you will\",\n",
        "    \"you'll've\": \"you shall have / you will have\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "cont_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
        "\n",
        "def expand(s, contractions=contractions):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return cont_re.sub(replace, s)\n",
        "\n",
        "# Function to remove emojis\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Function to apply all preprocessing steps\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^<]+?>', '', text)\n",
        "    # Expand contractions\n",
        "    text = expand(text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Remove punctuation\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    # Remove new line characters\n",
        "    text = re.sub('\\n', '', text)\n",
        "    # Remove emojis\n",
        "    text = remove_emojis(text)\n",
        "    # Lemmatize words\n",
        "    text = \" \".join([Word(word).lemmatize() for word in text.split()])\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to each sentence\n",
        "preprocessed_sentences = [preprocess_text(sentence) for sentence in sentence_list]\n",
        "\n",
        "# Verify the results\n",
        "print(preprocessed_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkcY-2z-aHb9"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure that the device is set to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "total_responses2 = []\n",
        "\n",
        "for sentence in tqdm(preprocessed_sentences, desc=\"Processing sentences\"):\n",
        "  # Transform to conversational format\n",
        "  test_dataset = create_input_prompt(sentence)\n",
        "\n",
        "  # Assuming 'test_dataset' is defined and accessible\n",
        "  prompt = pipe.tokenizer.apply_chat_template(test_dataset[\"messages\"][:2],\n",
        "                                              tokenize=False,\n",
        "                                              add_generation_prompt=True)\n",
        "  # Generate output using pipeline\n",
        "  outputs = pipe(prompt,\n",
        "                max_new_tokens=128,\n",
        "                do_sample=False,\n",
        "                temperature=0.1,\n",
        "                top_k=50,\n",
        "                top_p=0.1,\n",
        "                #device=device,\n",
        "                )\n",
        "  #print(outputs[0]['generated_text'])\n",
        "  total_responses2.append(outputs[0]['generated_text'])\n",
        "\n",
        "\n",
        "# Define the path to the file in Google Drive\n",
        "file_path = '/content/drive/MyDrive/outputresponse.txt'\n",
        "\n",
        "# Open the file in write mode and write the variable's value to it\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(repr(total_responses2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmiYlG3uabhn"
      },
      "outputs": [],
      "source": [
        "def parse_response(response):\n",
        "    \"\"\"\n",
        "    Parses the model's response into triples of (subject, relationship, object).\n",
        "    Example response: 'streets|were wet|this morning'\n",
        "    Returns: [('streets', 'were wet', 'this morning')]\n",
        "    \"\"\"\n",
        "    # Assuming each response is a single line triple separated by '|'\n",
        "    parts = response.strip().split('|')\n",
        "    if len(parts) == 3:\n",
        "        return [(parts[0].strip(), parts[1].strip(), parts[2].strip().replace('.',''))]\n",
        "    return []\n",
        "\n",
        "def extract_assistant_responses(text):\n",
        "    \"\"\"\n",
        "    Extracts lines following 'assistant' from a given block of text.\n",
        "    Args:\n",
        "    text (str): Multi-line string containing various roles and responses.\n",
        "\n",
        "    Returns:\n",
        "    list of str: List of responses that come directly after 'assistant'.\n",
        "    \"\"\"\n",
        "    lines = text.split('\\n')\n",
        "    assistant_line = \"\"\n",
        "    capture_next_line = False\n",
        "\n",
        "    for line in lines:\n",
        "        if capture_next_line:\n",
        "            assistant_line = line\n",
        "            break\n",
        "        if 'assistant' in line:\n",
        "            capture_next_line = True\n",
        "\n",
        "    return assistant_line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU57VYp77umJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qohw3fs73KS"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/outputresponse.txt'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    total_responses = eval(file.read())  # Use eval to parse the string representation of the list\n",
        "\n",
        "# Alternatively, you can use ast.literal_eval for safer parsing\n",
        "import ast\n",
        "with open(file_path, 'r') as file:\n",
        "    total_responses = ast.literal_eval(file.read())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MRyB9r3afpL"
      },
      "outputs": [],
      "source": [
        "responses = []\n",
        "\n",
        "for response in total_responses:\n",
        "  responses.append(extract_assistant_responses(response))\n",
        "\n",
        "responses2 = []\n",
        "\n",
        "for response in responses:\n",
        "  # Split the response by the delimiter '|'\n",
        "  triples = response.split('|')\n",
        "\n",
        "  # Reconstruct the first triple from the split parts\n",
        "  response = '|'.join(triples[:3])\n",
        "  responses2.append(parse_response(response))\n",
        "\n",
        "combined_list = [item for sublist in responses2 for item in sublist]\n",
        "\n",
        "len(combined_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kG--DDsAyKY"
      },
      "source": [
        "Import Libraries and Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn67et3EA32m"
      },
      "outputs": [],
      "source": [
        "pip install node2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05pKKJv8BA4Y"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8UMejKmAylS"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import openai\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "    return outputs['last_hidden_state'][:, 0, :].numpy()\n",
        "\n",
        "def cosine_similarity_bert(s1, s2):\n",
        "    emb1 = get_bert_embedding(s1)\n",
        "    emb2 = get_bert_embedding(s2)\n",
        "    similarity = cosine_similarity(emb1, emb2)[0][0]\n",
        "    return similarity\n",
        "\n",
        "# Load Spacy model for NLP\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYeZXgjRBHYZ"
      },
      "source": [
        "OpenAI API Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EAdiboQWBH6P"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"\" #Include the API Key from OpenAI site\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH0V15iLtm_x"
      },
      "source": [
        "Define Functions for Knowledge Graph and Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZTEJbtItna8"
      },
      "outputs": [],
      "source": [
        "# Create a knowledge graph from the triples\n",
        "def create_kg_from_triples(total_responses):\n",
        "    kg = nx.Graph()\n",
        "    for response in total_responses:\n",
        "        triples = response.split('assistant\\n')[1].split('\\n')\n",
        "        for triple in triples:\n",
        "            if '|' in triple:\n",
        "                parts = triple.split('|')\n",
        "                if len(parts) == 3:\n",
        "                    entity1, relationship, entity2 = parts\n",
        "                    kg.add_edge(entity1.strip(), entity2.strip(), relation=relationship.strip())\n",
        "    return kg\n",
        "\n",
        "# Generate Node2Vec embeddings for the graph\n",
        "def generate_node2vec_embeddings(kg):\n",
        "    node2vec = Node2Vec(kg, dimensions=64, walk_length=30, num_walks=200, workers=2)\n",
        "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
        "    node_embeddings = {node: model.wv[node] for node in kg.nodes}\n",
        "    return node_embeddings, model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGH16Lh8tpQD"
      },
      "source": [
        "Define Functions for Query Processing and Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj8dqmbrtrSI"
      },
      "outputs": [],
      "source": [
        "# Modified function to get similar nodes with BERT embedding similarity\n",
        "def get_similar_nodes_with_neighbors(query, node_embeddings, model, kg, top_n=10, threshold=0.35):\n",
        "    query_embedding = get_bert_embedding(query)\n",
        "\n",
        "    similarities = {}\n",
        "    for node, emb in node_embeddings.items():\n",
        "        node_embedding = get_bert_embedding(node)\n",
        "        similarity = cosine_similarity(query_embedding, node_embedding)[0][0]\n",
        "        if similarity > threshold:\n",
        "            similarities[node] = similarity\n",
        "\n",
        "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "    similar_nodes = [node for node, sim in sorted_similarities[:top_n]]\n",
        "\n",
        "    expanded_nodes = set(similar_nodes)\n",
        "    for node in similar_nodes:\n",
        "        node_embedding = node_embeddings[node].reshape(1, -1)\n",
        "        neighbor_similarities = {}\n",
        "        for other_node, other_emb in node_embeddings.items():\n",
        "            if other_node not in expanded_nodes:\n",
        "                similarity = 1 - cosine_similarity(node_embedding, other_emb.reshape(1, -1))\n",
        "                if similarity > threshold:\n",
        "                    neighbor_similarities[other_node] = similarity\n",
        "        sorted_neighbor_similarities = sorted(neighbor_similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "        top_neighbors = [neighbor for neighbor, sim in sorted_neighbor_similarities[:top_n]]\n",
        "        expanded_nodes.update(top_neighbors)\n",
        "\n",
        "    return list(expanded_nodes)\n",
        "\n",
        "def prompt_(prompt):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an experienced causal reasoner.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "    )\n",
        "    return completion.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vU7RXaJtsrq"
      },
      "source": [
        "Map Nodes to Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K70akBDJtvc3"
      },
      "outputs": [],
      "source": [
        "# Map nodes to sentences\n",
        "def map_nodes_to_sentences(nodes, preprocessed_sentences):\n",
        "    node_sentence_map = {}\n",
        "    for sentence in preprocessed_sentences:\n",
        "        entities = [token.text for token in nlp(sentence) if token.ent_type_]\n",
        "        for entity in entities:\n",
        "            if entity in nodes:\n",
        "                if entity not in node_sentence_map:\n",
        "                    node_sentence_map[entity] = []\n",
        "                node_sentence_map[entity].append(sentence)\n",
        "    return node_sentence_map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gy1yjUwtw7n"
      },
      "source": [
        "Generate Answers with KG RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfmQUgcztyvd"
      },
      "outputs": [],
      "source": [
        "# Generate answers using KG RAG with updated get_similar_nodes_with_neighbors\n",
        "def generate_answers_with_kg_rag(queries, kg, node_embeddings, node2vec_model, preprocessed_sentences, tokenizer, model):\n",
        "    kg_rag_answers = []\n",
        "    for query in queries:\n",
        "        similar_nodes = get_similar_nodes_with_neighbors(query, node_embeddings, node2vec_model, kg)\n",
        "        node_sentence_map = map_nodes_to_sentences(similar_nodes, preprocessed_sentences)\n",
        "        kg_context_sentences = [sentence for sentences in node_sentence_map.values() for sentence in sentences]\n",
        "        kg_context = \" \".join(kg_context_sentences)\n",
        "\n",
        "        prompt = f\"You are an experienced causal reasoner. Look at the query and infer cause for the query based on the provided context. Don't consider anything else. And don't say anything else, just provide the most probable cause. Query: {query} Context: {kg_context}\"\n",
        "\n",
        "        answer = prompt_(prompt)\n",
        "        kg_rag_answers.append(answer)\n",
        "        print(f\"KG Context: '{kg_context}'\")\n",
        "        print(f\"KG RAG Answer for query '{query}': {answer}\")\n",
        "    return kg_rag_answers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmrSLcxat0YX"
      },
      "source": [
        "Main Function to Run the Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNMx_8wot2tY"
      },
      "outputs": [],
      "source": [
        "# Main function to run the entire process\n",
        "def main(total_responses, queries, preprocessed_sentences):\n",
        "    # Create KG from triples\n",
        "    kg = create_kg_from_triples(total_responses)\n",
        "    context2 = '.'.join(preprocessed_sentences)\n",
        "\n",
        "    # Generate Node2Vec embeddings\n",
        "    node_embeddings, node2vec_model = generate_node2vec_embeddings(kg)\n",
        "\n",
        "    # Generate answers using KG RAG\n",
        "    kg_rag_answers = generate_answers_with_kg_rag(queries, kg, node_embeddings, node2vec_model, preprocessed_sentences, tokenizer, model)\n",
        "\n",
        "    print(kg_rag_answers)\n",
        "\n",
        "\n",
        "# Define the queries\n",
        "queries = [\n",
        "\n",
        "    # Add other queries here...\n",
        "]\n",
        "\n",
        "# Run the main function\n",
        "main(total_responses, queries, preprocessed_sentences)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
